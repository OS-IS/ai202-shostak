# Перевірка метафайлів веб-сервера на предмет витоку інформації

## Огляд

У цьому розділі описано, як перевірити різні файли метаданих на витік інформації про шлях(и) до веб-додатку або його функціональність. Крім того, список каталогів, яких слід уникати павукам, роботам або сканерам, також може бути створений як залежність для [відображення шляхів виконання через додаток](https://owasp.org/www-project-web-security-testing-guide/stable/4-Web_Application_Security_Testing/01-Information_Gathering/07-Map_Execution_Paths_Through_Application). Інша інформація також може бути зібрана для визначення поверхні атаки, деталей технології або для використання в соціальній інженерії.

## Цілі тестування

- Виявити приховані або заплутані шляхи та функціональність через аналіз файлів метаданих.

- Витягнути та нанести на карту іншу інформацію, яка може привести до кращого розуміння систем, що розглядаються.

 
## Як тестувати

Будь-яку з дій, описаних нижче за допомогою `wget`, можна також виконати за допомогою `curl`. Багато інструментів динамічного тестування безпеки додатків (DAST), таких як ZAP і Burp Suite, включають в себе перевірку або синтаксичний аналіз цих ресурсів як частину функціональності павука/сканера. Їх також можна ідентифікувати за допомогою різних [Google Dorks](https://en.wikipedia.org/wiki/Google_hacking) або за допомогою розширених функцій пошуку, таких як `inurl:`.

## Роботи

Веб-павуки, роботи або сканери отримують веб-сторінку, а потім рекурсивно перебирають гіперпосилання, щоб отримати подальший веб-вміст. Їх прийнятна поведінка визначається [протоколом виключення роботів](https://www.robotstxt.org/) у файлі [robots.txt](https://www.robotstxt.org/) в кореневому каталозі сайту.

Як приклад, нижче наведено початок файлу `robots.txt` від [Google](https://www.google.com/robots.txt), вибірка якого зроблена 5 травня 2020 року:

```
Користувач-агент: *
Заборонити: /search
Дозволити: /search/about
Дозволити: /search/static
Дозволити: /search/howsearchworks
Заборонити: /sdch
...
```

Директива [User-Agent](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent) посилається на конкретного веб-павука/робота/сканера. Наприклад, `User-Agent: Googlebot` вказує на павука від Google, а `User-Agent: bingbot` - на робота від Microsoft. `User-Agent: *` у наведеному вище прикладі застосовується до всіх [веб-павуків/роботів/сканерів](https://support.google.com/webmasters/answer/6062608?visit_id=637173940975499736-3548411022&rd=1).

Директива `Disallow` визначає, які ресурси заборонено павукам/роботам/сканерам. У наведеному вище прикладі заборонені наступні ресурси:

```
...
Заборонити: /search
...
Заборонити: /sdch
...
```
 
Веб-павуки/роботи/сканери можуть [навмисно ігнорувати](https://blog.isc2.org/isc2_blog/2008/07/the-attack-of-t.html) директиви `Disallow`, зазначені у файлі `robots.txt`, наприклад, від [соціальних мереж](https://www.htbridge.com/news/social_networks_can_robots_violate_user_privacy.html), щоб переконатися, що спільні посилання залишаються дійсними. Таким чином, файл `robots.txt` не слід розглядати як механізм для введення обмежень на доступ до веб-контенту, його зберігання або повторну публікацію третіми особами.

Файл `robots.txt` завантажується з кореневого каталогу веб-сервера. Наприклад, щоб отримати файл `robots.txt` з `www.google.com`, використовуйте `wget` або `curl`:

``` 
$ curl -O -Ss http://www.google.com/robots.txt && head -n5 robots.txt
Користувач-агент: *
Заборонити: /search
Дозволити: /search/about
Дозволити: /search/static
Дозволити: /search/howsearchworks
...
```

### Аналіз robots.txt за допомогою Інструментів для веб-майстрів Google

Власники веб-сайтів можуть використовувати функцію Google «Analyze robots.txt» для аналізу веб-сайту в рамках [Інструментів для веб-майстрів Google](https://www.google.com/webmasters/tools). Цей інструмент може допомогти в тестуванні, а процедура виглядає наступним чином:

1. Увійдіть в Інструменти для веб-майстрів Google за допомогою облікового запису Google.

2. На інформаційній панелі введіть URL-адресу сайту, який потрібно проаналізувати.

3. Виберіть один з доступних методів і дотримуйтесь інструкцій на екрані.

## МЕТА-теги

Теги `<META>` розташовуються в розділі `HEAD` кожного HTML-документа і повинні бути узгодженими на всьому веб-сайті, якщо початкова точка запуску робота/павука/сканера не починається з посилання на документ, відмінного від webroot, тобто [глибокого посилання](https://en.wikipedia.org/wiki/Deep_linking). Директиву для роботів також можна вказати за допомогою спеціального [META-тега](https://www.robotstxt.org/meta.html).

### Мета-тег Robots

Якщо немає запису `<META NAME=«ROBOTS» ... >`, то «Протокол виключення роботів» за замовчуванням має значення `INDEX,FOLLOW` відповідно. Таким чином, інші два допустимі записи, визначені «Протоколом виключення роботів», мають префікс `NO...`, тобто `NOINDEX` і `NOFOLLOW`.

На основі директив Disallow, перелічених у файлі `robots.txt` у webroot, виконується пошук за регулярним виразом `<META NAME=«ROBOTS»>` на кожній веб-сторінці, а результат порівнюється з файлом `robots.txt` у webroot.

### Різні інформаційні МЕТА-теги

Організації часто вбудовують інформаційні МЕТА-теги у веб-контент для підтримки різних технологій, таких як зчитування з екрану, попередній перегляд у соціальних мережах, індексація пошуковими системами тощо. Така мета-інформація може бути корисною для тестувальників у визначенні використаних технологій, а також додаткових шляхів/функціональних можливостей для дослідження і тестування. Наступна метаінформація була отримана з `www.whitehouse.gov` за допомогою View Page Source 05 травня 2020 року:

```
...
<meta property=«og:locale» content=«en_US» /> <meta property=«og:locale» content=«en_US» />
<meta property=«og:type» content=«website» /> <meta property=«og:type» content=«website» />
<meta property=«og:title» content=«The White House» />
<meta property=«og:description» content="Ми, громадяни Америки, зараз об'єднані у великих національних зусиллях, щоб відбудувати нашу країну і відновити її обіцянки для всіх. - Президент Дональд Трамп." />
<meta property=«og:url» content=«https://www.whitehouse.gov/» />
<meta property=«og:site_name» content=«The White House» />
<meta property=«fb:app_id» content=«1790466490985150» />
<meta property=«og:image» content=«https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png» />
<meta property=«og:image:secure_url» content=«https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png» />
<meta name=«twitter:card» content=«summary_large_image» />
<meta name=«twitter:description» content="Ми, громадяни Америки, зараз об'єднані у великих національних зусиллях, щоб відбудувати нашу країну і відновити її обіцянки для всіх. - Президент Дональд Трамп." />
<meta name=«twitter:title» content=«Білий дім» />
<meta name=«twitter:site» content=«@whitehouse» />
<meta name=«twitter:image» content=«https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png» />
<meta name=«twitter:creator» content=«@whitehouse» />
...
<meta name=«apple-mobile-web-app-title» content=«The White House»>
<meta name=«application-name» content=«The White House»>
<meta name=«msapplication-TileColor» content=«#0c2644»>
<meta name=«theme-color» content=«#f5f5f5»>
...
```

### Карти сайту

Карта сайту - це файл, в якому розробник або організація може надати інформацію про сторінки, відео та інші файли, пропоновані сайтом або додатком, а також про взаємозв'язок між ними. Пошукові системи можуть використовувати цей файл, щоб краще досліджувати ваш сайт. Тестувальники можуть використовувати файли `sitemap.xml`, щоб дізнатися більше про сайт або додаток, щоб вивчити його більш повно.

Наступний уривок взятий з основної карти сайту Google, отриманої 05 травня 2020 року.

```
 
$ wget --no-verbose https://www.google.com/sitemap.xml && head -n8 sitemap.xml
2020-05-05 12:23:30 URL:https://www.google.com/sitemap.xml [2049] -> «sitemap.xml» [1].

<?xml version=«1.0» encoding=«UTF-8»?
<sitemapindex xmlns=«http://www.google.com/schemas/sitemap/0.84»>
  <sitemap
    <loc>https://www.google.com/gmail/sitemap.xml</loc>
  </sitemap></sitemap
  <мапа сайту> </p> <мапа сайту> </p> <sitemap> </p> <sitemap
    <loc>https://www.google.com/forms/sitemaps.xml</loc>
  </sitemap>
...
```

Досліджуючи його, тестувальник може захотіти отримати карту сайту gmail https://www.google.com/gmail/sitemap.xml:

```
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" xmlns:xhtml="http://www.w3.org/1999/xhtml">
  <url>
    <loc>https://www.google.com/intl/am/gmail/about/</loc>
    <xhtml:link href="https://www.google.com/gmail/about/" hreflang="x-default" rel="alternate"/>
    <xhtml:link href="https://www.google.com/intl/el/gmail/about/" hreflang="el" rel="alternate"/>
    <xhtml:link href="https://www.google.com/intl/it/gmail/about/" hreflang="it" rel="alternate"/>
    <xhtml:link href="https://www.google.com/intl/ar/gmail/about/" hreflang="ar" rel="alternate"/>
...
```

### Безпека TXT

`security.txt` - це [запропонований стандарт](https://securitytxt.org/), який дозволяє веб-сайтам визначати політику безпеки та контактні дані. Існує багато причин, чому це може представляти інтерес для тестових сценаріїв, включаючи, але не обмежуючись ними:

- Визначення подальших шляхів або ресурсів для включення у виявлення/аналіз.
- Збір розвідувальних даних з відкритим кодом.
- Пошук інформації про винагороди за виправлення помилок тощо.
- Соціальна інженерія.

Файл може знаходитися або в корені веб-сервера, або в каталозі `.well-known/`. Наприклад:

- https://example.com/security.txt
- https://example.com/.well-known/security.txt

Ось приклад з реального світу, взятий з LinkedIn 2020 травня 05:

```
$ wget --no-verbose https://www.linkedin.com/.well-known/security.txt && cat security.txt
2020-05-07 12:56:51 URL:https://www.linkedin.com/.well-known/security.txt [333/333] -> «security.txt» [1].
# Відповідає IETF `draft-foudil-securitytxt-07`.
Контакти: mailto:security@linkedin.com
Контакт: https://www.linkedin.com/help/linkedin/answer/62924
Шифрування: https://www.linkedin.com/help/linkedin/answer/79676
Канонічний: https://www.linkedin.com/.well-known/security.txt
Політика: https://www.linkedin.com/help/linkedin/answer/62924
```

### Люди TXT

`humans.txt` - це ініціатива, що дозволяє дізнатися про людей, які стоять за веб-сайтом. Це текстовий файл, який містить інформацію про різних людей, які зробили свій внесок у створення веб-сайту. Дивіться [humanstxt](http://humanstxt.org/) для отримання додаткової інформації. Цей файл часто (але не завжди) містить інформацію для сайтів/шляхів кар'єри чи працевлаштування.

Наступний приклад було отримано з Google 2020 травня 05:

```
$ wget --no-verbose https://www.google.com/humans.txt && cat humans.txt
2020-05-07 12:57:52 URL:https://www.google.com/humans.txt [286/286] -> «humans.txt» [1].
Google створюється великою командою інженерів, дизайнерів, дослідників, роботів та інших людей на багатьох різних майданчиках по всьому світу. Він постійно оновлюється і створений за допомогою більшої кількості інструментів і технологій, ніж ми можемо перерахувати. Якщо ви хочете нам допомогти, відвідайте careers.google.com.
```

### Інші джерела інформації у домені .well-known

Існують інші документи RFC та інтернет-проекти, які пропонують стандартизоване використання файлів у каталозі `.well-known/`. Списки цих документів можна знайти [тут](https://en.wikipedia.org/wiki/List_of_/.well-known/_services_offered_by_webservers) або [тут](https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml).

Для тестувальника буде досить просто переглянути RFC/чернетки і створити список, який буде передано пошуковику або фаззеру, щоб перевірити існування або вміст таких файлів.

## Інструменти

- Браузер (перегляд вихідного коду або інструментів розробника)
- curl
- wget
- Burp Suite
- ZAP

